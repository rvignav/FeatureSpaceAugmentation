{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Space_Aug.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaykurani/DataAugmentation/blob/master/Feature_Space_Aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjJZCoVA0wBz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx5ZmUe2APlM"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import math\n",
        "import numbers\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "from torchsummary import summary\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpL_BcvjIopL"
      },
      "source": [
        "n_epochs = 20\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 1e-4\n",
        "log_interval = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwKUEVDJIrbe"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.5,), (0.5,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.5,), (0.5,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "test_data = torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.5,), (0.5,))\n",
        "                             ]))\n",
        "\n",
        "inp, _ = test_data[0]\n",
        "inp = inp.unsqueeze(0)\n",
        "inp = inp.cpu().detach().numpy()[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xILZZTsI5qR"
      },
      "source": [
        "'''\n",
        "Information on interpolation here: https://medium.com/@wenrudong/what-is-opencvs-inter-area-actually-doing-282a626a09b3\n",
        "'''\n",
        "\n",
        "def dot(inp, x):\n",
        "  image = cv2.resize(inp, (x[0][0].shape[1], x[0][0].shape[0]), \n",
        "                     interpolation = cv2.INTER_AREA)\n",
        "  sum = np.zeros(x[0][0].shape)\n",
        "  for b in range(x.shape[0]):\n",
        "    for f in range(x.shape[1]):\n",
        "      sum += image.dot(x[b][f])\n",
        "  return sum\n",
        "\n",
        "class Net(nn.Module):\n",
        "    '''\n",
        "    Adapted from https://github.com/sksq96/pytorch-summary.\n",
        "    More information on layers available here: https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(4608, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        For the first layer (add more layers based on results), \n",
        "        (1) rotate by 15 or 30 deg increments through 90 to get a dictionary of feature spaces\n",
        "        (2) for each of those rotated spaces, take a dot product of the input image and the current layer \n",
        "            to figure out which gives the maximal activation or highest correlation\n",
        "        (3) pass that maximally correlated feature space to the next layer\n",
        "        '''\n",
        "        c1 = []\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        xn = x.cpu().detach().numpy()\n",
        "        for i in range(1,7):\n",
        "          deg = 15 * i\n",
        "          xp = x.detach().clone().cpu().numpy()\n",
        "          for b in range(xp.shape[0]):\n",
        "            for f in range(xp.shape[1]):\n",
        "              xp[b][f] = ndimage.rotate(xp[b][f], deg, reshape=False)\n",
        "          c1.append(xp)\n",
        "        for xp in c1:\n",
        "          arr = np.greater_equal(dot(inp, xp), dot(inp, xn))\n",
        "          count = 0\n",
        "          tot = 0\n",
        "          for r in range(arr.shape[0]):\n",
        "            for c in range(arr.shape[1]):\n",
        "              tot += 1\n",
        "              if arr[r][c]:\n",
        "                count += 1\n",
        "          if count > tot // 2:\n",
        "            x = torch.from_numpy(xp).cuda()\n",
        "            xn = x.cpu().detach().numpy()\n",
        "       \n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def loss_function(self, out, target):\n",
        "        return F.cross_entropy(out, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYrbmIPsJGzD"
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "network = Net()\n",
        "network.apply(init_weights)\n",
        "network.cuda()\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(), lr=1e-4)\n",
        "\n",
        "summary(network, (1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG1xq16CLKBR"
      },
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data = data.cuda()\n",
        "    target = target.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = network.loss_function(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2CJB0owLJ-6"
      },
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data = data.cuda()\n",
        "      target = target.cuda()\n",
        "      target = target.view(batch_size_test)\n",
        "      output = network(data)\n",
        "      test_loss += network.loss_function(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9mxo7d4LXuT"
      },
      "source": [
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRq5CDkdA7k_"
      },
      "source": [
        "torch.save(network.state_dict(), 'network_fsaug.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU-515UNLZUJ"
      },
      "source": [
        "for data, target in test_loader:\n",
        "  im = data[0]\n",
        "  im = torch.squeeze(im)\n",
        "  plt.imshow(im.numpy())\n",
        "  plt.show()\n",
        "  data = data.cuda()\n",
        "  target = target.cuda()\n",
        "  output = network(data)\n",
        "  pred = output.data.max(1, keepdim=True)[1]\n",
        "  print(\"Prediction: \" + str(pred[0].item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBqsTnYnSX0R"
      },
      "source": [
        "'''\n",
        "Adapted from https://towardsdatascience.com/visualizing-convolution-neural-networks-using-pytorch-3dfa8443e74e.\n",
        "More information on feature visualization: https://distill.pub/2017/feature-visualization/\n",
        "'''\n",
        "\n",
        "def imshow(img, title):\n",
        "  std_correction = np.asarray([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "  mean_correction = np.asarray([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "  npimg = np.multiply(img.numpy(), std_correction) + mean_correction\n",
        "  plt.figure(figsize = (batch_size * 4, 4))\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "def plot_filters_single_channel_big(t):\n",
        "    \n",
        "    #setting the rows and columns\n",
        "    nrows = t.shape[0]*t.shape[2]\n",
        "    ncols = t.shape[1]*t.shape[3]\n",
        "    \n",
        "    \n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    npimg = npimg.transpose((0, 2, 1, 3))\n",
        "    npimg = npimg.ravel().reshape(nrows, ncols)\n",
        "    \n",
        "    npimg = npimg.T\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(ncols/10, nrows/200))    \n",
        "    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)\n",
        "  \n",
        "def plot_filters_single_channel(t):\n",
        "    \n",
        "    #kernels depth * number of kernels\n",
        "    nplots = t.shape[0]*t.shape[1]\n",
        "    ncols = 12\n",
        "    \n",
        "    nrows = 1 + nplots//ncols\n",
        "    #convert tensor to numpy image\n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    \n",
        "    count = 0\n",
        "    fig = plt.figure(figsize=(ncols, nrows))\n",
        "    \n",
        "    #looping through all the kernels in each channel\n",
        "    for i in range(t.shape[0]):\n",
        "        for j in range(t.shape[1]):\n",
        "            count += 1\n",
        "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
        "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
        "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "            ax1.imshow(npimg)\n",
        "            ax1.set_title(str(i))\n",
        "            ax1.axis('off')\n",
        "            ax1.set_xticklabels([])\n",
        "            ax1.set_yticklabels([])\n",
        "   \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_filters_multi_channel(t):\n",
        "    \n",
        "    #get the number of kernals\n",
        "    num_kernels = t.shape[0]    \n",
        "    \n",
        "    #define number of columns for subplots\n",
        "    num_cols = 12\n",
        "    #rows = num of kernels\n",
        "    num_rows = num_kernels\n",
        "    \n",
        "    #set the figure size\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "    #looping through all the kernels\n",
        "    for i in range(t.shape[0]):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "        #for each kernel, we convert the tensor to numpy \n",
        "        npimg = np.array(t[i].numpy(), np.float32)\n",
        "        #standardize the numpy image\n",
        "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "        npimg = npimg.transpose((1, 2, 0))\n",
        "        ax1.imshow(npimg)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(str(i))\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "        \n",
        "    plt.savefig('myimage.png', dpi=100)    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_weights(model, layer_num, single_channel = True, collated = False):\n",
        "  #extracting the model features at the particular layer number\n",
        "  if layer_num == 1:\n",
        "    layer = model.conv1\n",
        "  elif layer_num == 2:\n",
        "    layer = model.conv2\n",
        "  else:\n",
        "    print(\"Convolutional layer not found\")\n",
        "    return\n",
        "  \n",
        "  #checking whether the layer is convolution layer or not \n",
        "  if isinstance(layer, nn.Conv2d):\n",
        "    #getting the weight tensor data\n",
        "    weight_tensor = layer.weight.data\n",
        "    \n",
        "    if single_channel:\n",
        "      if collated:\n",
        "        plot_filters_single_channel_big(weight_tensor)\n",
        "      else:\n",
        "        plot_filters_single_channel(weight_tensor)\n",
        "        \n",
        "    else:\n",
        "      if weight_tensor.shape[1] == 3:\n",
        "        plot_filters_multi_channel(weight_tensor)\n",
        "      else:\n",
        "        print(\"Can only plot weights with three channels with single channel = False\")\n",
        "        \n",
        "  else:\n",
        "    print(\"Can only visualize layers which are convolutional\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhWmHFL_3oKI"
      },
      "source": [
        "### **Convolutional Filter Visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-5D1D1_Tp4W"
      },
      "source": [
        "plot_weights(network.cpu(), 1, single_channel = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCHfdKgV3lwO"
      },
      "source": [
        "### **Feature Maps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5_DBhjQhrAx"
      },
      "source": [
        "'''\n",
        "Adapted from https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/.\n",
        "'''\n",
        "\n",
        "img = cv2.imread(\"/content/drive/My Drive/six.png\", 0)\n",
        "img = cv2.bitwise_not(img)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "img = np.array(img)\n",
        "\n",
        "img = transform(img)\n",
        "\n",
        "plt.imshow(img.squeeze(0))\n",
        "plt.show()\n",
        "\n",
        "img = img.unsqueeze(0)\n",
        "\n",
        "results = [F.relu(F.max_pool2d(network.conv1(img), 2))]\n",
        "results.append(F.relu(F.max_pool2d(network.conv2(results[0]), 2)))\n",
        "\n",
        "outputs = results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpWApGE_4jue"
      },
      "source": [
        "num_layer = 0\n",
        "plt.figure(figsize=(10, 10))\n",
        "layer_viz = outputs[num_layer][0, :, :, :]\n",
        "layer_viz = layer_viz.data\n",
        "\n",
        "filters = []\n",
        "\n",
        "n = 64\n",
        "sq = int(math.sqrt(n))\n",
        "\n",
        "for i, filter in enumerate(layer_viz):\n",
        "    plt.subplot(sq, sq, i + 1)\n",
        "    filters.append(filter)\n",
        "    plt.imshow(filter, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpfL3qyY4kPq"
      },
      "source": [
        "num_layer = 1\n",
        "plt.figure(figsize=(10, 10))\n",
        "layer_viz = outputs[num_layer][0, :, :, :]\n",
        "layer_viz = layer_viz.data\n",
        "\n",
        "n = 256\n",
        "sq = int(math.sqrt(n))\n",
        "\n",
        "for i, filter in enumerate(layer_viz):\n",
        "    plt.subplot(8, sq, i + 1)\n",
        "    plt.imshow(filter, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoJHSOa43Sv3"
      },
      "source": [
        "### **Maximum Activation Visualizations**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FhRn0emckuT"
      },
      "source": [
        "'''\n",
        "Adapted from https://discuss.pytorch.org/t/visualize-feature-map/29597.\n",
        "'''\n",
        "\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "dataset = torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.5,), (0.5,))\n",
        "                             ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EA_h1_ZoH8C"
      },
      "source": [
        "network.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = dataset[0]\n",
        "data.unsqueeze_(0)\n",
        "output = network(data)\n",
        "\n",
        "n = 64\n",
        "sq = int(math.sqrt(n))\n",
        "\n",
        "act = activation['conv1'].squeeze()\n",
        "\n",
        "fig, axarr = plt.subplots(sq, sq, figsize=(10,10))\n",
        "for idx in range(act.size(0)):\n",
        "    axarr[idx // sq][idx % sq].axis('off')\n",
        "    axarr[idx // sq][idx % sq].imshow(act[idx], cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BgMMQZUduC0"
      },
      "source": [
        "network.conv2.register_forward_hook(get_activation('conv2'))\n",
        "data, _ = dataset[0]\n",
        "\n",
        "data.unsqueeze_(0)\n",
        "output = network(data)\n",
        "\n",
        "act = activation['conv2'].squeeze()\n",
        "\n",
        "n = 128\n",
        "\n",
        "fig, axarr = plt.subplots(8, 16, figsize=(10,10))\n",
        "for idx in range(act.size(0)):\n",
        "    axarr[idx // 16][idx % 16].axis('off')\n",
        "    axarr[idx // 16][idx % 16].imshow(act[idx], cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}